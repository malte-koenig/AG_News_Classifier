{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a1c519b",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a03b45c",
   "metadata": {},
   "source": [
    "#### paste following lines into anaconda prompt (as admin) and press enter: \n",
    "#### conda install -c huggingface -c conda-forge datasets\n",
    "#### conda install -c anaconda gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be193bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/zeynep/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/zeynep/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/zeynep/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/zeynep/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import re, string, nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec, FastText\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e5a1ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (/Users/zeynep/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c666ddcd79342b9833fc0d5cbc45ae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#load the data set: text & labels [World (0), Sports (1), Business (2), Sci/Tech (3)]\n",
    "dataset = load_dataset(\"ag_news\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d03c42a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(dataset['train'])\n",
    "train_df['text'] = train_df['text'].apply(lambda x: preprocessing(x))\n",
    "train_df.to_csv('training_data.csv', sep=';', encoding='utf-8', index=False)\n",
    "train_df.head()\n",
    "\n",
    "\n",
    "test_df = pd.DataFrame(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d721d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize stopwords and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def preprocessing(text):\n",
    "    \n",
    "    # convert to lowercase and remove spaces at beginning and ending\n",
    "    text = text.lower().strip()\n",
    "    \n",
    "    # remove punctuation\n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)\n",
    "    \n",
    "    # remove html code\n",
    "    text= re.sub('<.*?>', '', text) \n",
    "    \n",
    "    # remove special characters\n",
    "    text=re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    \n",
    "    # remove digits\n",
    "    text = re.sub(r'\\d',' ',text)\n",
    "    \n",
    "    # replace multiple whitespaces with one\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    \n",
    "    # stop word removal\n",
    "    clean_text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    \n",
    "    # tonkenize & lemmatize\n",
    "    word_pos_tags = nltk.pos_tag(word_tokenize(clean_text)) # -> list of tuples (word, pos_tag) [('computer', 'NN'), ('word', 'tag')]\n",
    "    lem_text = ' '.join([wnl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for tag in word_pos_tags])\n",
    "\n",
    "    return lem_text\n",
    "\n",
    " \n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        # default pos\n",
    "        return wordnet.NOUN\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d47b244a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wall st bear claw back black reuters reuters s...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>carlyle look toward commercial aerospace reute...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>oil economy cloud stock outlook reuters reuter...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>iraq halt oil export main southern pipeline re...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>oil price soar time record pose new menace u e...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  wall st bear claw back black reuters reuters s...      2\n",
       "1  carlyle look toward commercial aerospace reute...      2\n",
       "2  oil economy cloud stock outlook reuters reuter...      2\n",
       "3  iraq halt oil export main southern pipeline re...      2\n",
       "4  oil price soar time record pose new menace u e...      2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# execute preprocessing for training set\n",
    "train_df['text'] = train_df['text'].apply(lambda x: preprocessing(x))\n",
    "train_df.to_csv('training_data.csv', sep=';', encoding='utf-8', index=False)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf6fb62",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7b24275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in preprocessed training data if necessary\n",
    "train_df = pd.read_csv('preprocessed_training_data.csv', sep=';', encoding='utf-8')\n",
    "# for word embedding models\n",
    "train_tokens = [word_tokenize(text) for text in train_df['text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9387d88e",
   "metadata": {},
   "source": [
    "### Count vectors and Tf-idf vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb9c4bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "count_vectorizer = CountVectorizer()\n",
    "tfidf_vectors = tfidf_vectorizer.fit_transform(train_df['text'])\n",
    "count_vectors = count_vectorizer.fit_transform(train_df['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc87a9f",
   "metadata": {},
   "source": [
    "### Word2Vec SkipGram & CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4ae686c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_cbow = Word2Vec(train_tokens, min_count=2, vector_size=300, window=5)\n",
    "w2v_skipg = Word2Vec(train_tokens, min_count=2, vector_size=300, window=5, sg = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "431909b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns mean w2v vector for list of specified words\n",
    "def get_embedding(model, text):\n",
    "    existing_words = [word for word in text if word in model.wv.key_to_index]\n",
    "    if existing_words:\n",
    "        embedding = np.zeros((len(existing_words), model.vector_size), dtype=np.float32)\n",
    "        for i, w in enumerate(existing_words):\n",
    "                embedding[i] = model.wv[w]\n",
    "        return np.mean(embedding, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "783182c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mean vector for each article description for both models\n",
    "embeddings_w2v_cbow = np.array([get_embedding(w2v_cbow, text) for text in train_tokens])\n",
    "embeddings_w2v_skipg = np.array([get_embedding(w2v_skipg, text) for text in train_tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20988705",
   "metadata": {},
   "source": [
    "### fastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "02f27af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30119724, 31006020)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext = FastText(vector_size=300, window=5, min_count=2)\n",
    "fasttext.build_vocab(corpus_iterable=train_tokens)\n",
    "fasttext.train(corpus_iterable=train_tokens, total_examples=len(train_tokens), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f1714253",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_fasttext = np.array([get_embedding(fasttext, text) for text in train_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac4901c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
