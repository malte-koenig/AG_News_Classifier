{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a1c519b",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be193bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re, string, nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV, ParameterGrid\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, roc_curve, precision_recall_curve, auc, confusion_matrix\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from scipy import sparse\n",
    "import seaborn as sns\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235bdb01",
   "metadata": {},
   "source": [
    "# Train test split and class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03c42a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train-test split\n",
    "dataset = load_dataset(\"ag_news\")\n",
    "train_df = pd.DataFrame(dataset['train'])\n",
    "test_df = pd.DataFrame(dataset['test'])\n",
    "del dataset\n",
    "\n",
    "def class_distribution():\n",
    "    \n",
    "    # checking class distribution\n",
    "    plt.figure(figsize=(10,5))\n",
    "\n",
    "    ## train data\n",
    "    plt.subplot(1,2,1)\n",
    "    train_df_target = train_df['label']\n",
    "    class_dist = pd.Series(train_df_target).value_counts()\n",
    "    plt.title('train_df')\n",
    "    plt.bar(class_dist.index, class_dist)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    ## test data\n",
    "    plt.subplot(1,2,2)\n",
    "    test_df_target = test_df['label']\n",
    "    class_dist = pd.Series(test_df_target).value_counts()\n",
    "    plt.title('test_df')\n",
    "    plt.bar(class_dist.index, class_dist)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def balanced_fractionize(df, frac):\n",
    "    \n",
    "    frac_df = df.sample(frac=frac, random_state=42)\n",
    "    frac_data, frac_target = frac_df['text'].values.reshape(-1, 1), frac_df['label'].values.reshape(-1, 1)\n",
    "    \n",
    "    sampler = RandomUnderSampler()\n",
    "    bal_frac_data, bal_frac_target = sampler.fit_resample(frac_data, frac_target)\n",
    "    \n",
    "    class_dist = pd.Series(bal_frac_target).value_counts()\n",
    "    plt.bar(class_dist.index, class_dist)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "    \n",
    "    return pd.DataFrame(bal_frac_data, columns = ['text']), pd.DataFrame(bal_frac_target, columns = ['label'])\n",
    "\n",
    "class_distribution()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d721d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize stopwords and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "#get the wordnet POS tag\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        # default pos\n",
    "        return wordnet.NOUN\n",
    "\n",
    "#text preprocessing\n",
    "def preprocessing(text):\n",
    "    \n",
    "    # convert to lowercase and remove spaces at beginning and ending\n",
    "    text = text.lower().strip()\n",
    "    \n",
    "    # remove punctuation\n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)\n",
    "    \n",
    "    # remove html code\n",
    "    text= re.sub('<.*?>', '', text) \n",
    "    \n",
    "    # remove special characters\n",
    "    text=re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    \n",
    "    # remove digits\n",
    "    text = re.sub(r'\\d',' ',text)\n",
    "    \n",
    "    # replace multiple whitespaces with one\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    \n",
    "    # stop word removal\n",
    "    clean_text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    \n",
    "    # tonkenize & lemmatize\n",
    "    word_pos_tags = nltk.pos_tag(word_tokenize(clean_text)) # -> list of tuples (word, pos_tag) [('computer', 'NN'), ('word', 'tag')]\n",
    "    lem_text = ' '.join([wnl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for tag in word_pos_tags])\n",
    "\n",
    "    return lem_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47b244a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute preprocessing for training set\n",
    "train_df['text'] = train_df['text'].apply(lambda x: preprocessing(x))\n",
    "train_df.to_csv('training_data.csv', sep=';', encoding='utf-8', index=False)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb7d8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute preprocessing for test set\n",
    "test_df['text'] = test_df['text'].apply(lambda x: preprocessing(x))\n",
    "test_df.to_csv('preprocessed_test_data.csv', sep=';', encoding='utf-8', index=False)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf6fb62",
   "metadata": {},
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b970df",
   "metadata": {},
   "source": [
    "## Reading in the preprocessed training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b24275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in preprocessed training data if necessary\n",
    "train_df = pd.read_csv('preprocessed_training_data.csv', sep=';', encoding='utf-8')\n",
    "\n",
    "train_df.loc[(train_df.label == 0),'label']='World'\n",
    "train_df.loc[(train_df.label == 1),'label']='Sports'\n",
    "train_df.loc[(train_df.label == 2),'label']='Business'\n",
    "train_df.loc[(train_df.label == 3),'label']='Sci/Tech'\n",
    "\n",
    "train_df['label']= train_df['label'].astype(str)\n",
    "\n",
    "\n",
    "#fractionize and undersample data\n",
    "train_data, train_target = balanced_fractionize(train_df, 0.2)\n",
    "\n",
    "#Tokenize\n",
    "def get_train_tokens(df_data):\n",
    "    return [word_tokenize(text) for text in df_data['text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9387d88e",
   "metadata": {},
   "source": [
    "## Training the vectorizers on the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80957c09",
   "metadata": {},
   "source": [
    "### TF IDF & Count Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21f6fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_count_std_parameters = {\n",
    "    'max_features': 10000,\n",
    "}\n",
    "\n",
    "# tf idf\n",
    "vectorizer_tf_idf = TfidfVectorizer(**tf_count_std_parameters)\n",
    "tf_idf_vectors = vectorizer_tf_idf.fit_transform(train_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ab2197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count\n",
    "vectorizer_count = CountVectorizer(**tf_count_std_parameters)\n",
    "count_vectors = vectorizer_count.fit_transform(train_df['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc87a9f",
   "metadata": {},
   "source": [
    "### Word2Vec SkipGram & CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8aa31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wordEmbedding_std_parameters = {\n",
    "    'window': 8,\n",
    "    'vector_size': 100,\n",
    "    'min_count': 5\n",
    "}\n",
    "\n",
    "# get tokens\n",
    "tokens = get_train_tokens(train_df)\n",
    "\n",
    "# Train the models on the training data once\n",
    "\n",
    "#w2v_cbow = Word2Vec(tokens, **wordEmbedding_std_parameters)\n",
    "#w2v_skipg = Word2Vec(tokens, **wordEmbedding_std_parameters, sg = 1)\n",
    "\n",
    "#w2v_cbow.save(\"w2v_cbow.model\")\n",
    "#w2v_skipg.save(\"w2v_skipg.model\")\n",
    "\n",
    "w2v_cbow = Word2Vec.load(\"w2v_cbow.model\")\n",
    "w2v_skipg = Word2Vec.load(\"w2v_skipg.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20988705",
   "metadata": {},
   "source": [
    "### fastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f27af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model once on the training data\n",
    "fasttext = FastText(**wordEmbedding_std_parameters)\n",
    "fasttext.build_vocab(corpus_iterable=tokens)\n",
    "fasttext.train(corpus_iterable=tokens, total_examples=len(tokens), epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a43b149",
   "metadata": {},
   "source": [
    "### GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55d33d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write content into file\n",
    "# with open('corpus.txt', 'w') as f:\n",
    "#     for text in train_df['text'].tolist():\n",
    "#         f.write(text + '\\n')\n",
    "# train vectors with https://github.com/stanfordnlp/GloVe with standard parameters (window=8, size=100, MIN_COUNT=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d7debd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the vocab and the vectors after training\n",
    "vocab = []\n",
    "vectors = {}\n",
    "with open('vocab.txt') as f:\n",
    "    for ln in f:\n",
    "        words = ln.split()\n",
    "        vocab = vocab + words[:1]\n",
    "with open('vectors.txt') as f:\n",
    "    for ln in f:\n",
    "        word = ln.split()[0]\n",
    "        vector = [float(number) for number in ln.split()[1:]]\n",
    "        vectors[word] = vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfdd66a",
   "metadata": {},
   "source": [
    "## Functions for getting the vectors for a given data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9c4bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfid_vectors(df_data):\n",
    "    return vectorizer_tf_idf.transform(df_data['text'])\n",
    "\n",
    "def get_count_vectors(df_data):\n",
    "    return vectorizer_count.transform(df_data['text'])\n",
    "\n",
    "# returns mean vector for list of specified words\n",
    "def get_embedding(model, text):\n",
    "    existing_words = [word for word in text if word in list(model.wv.index_to_key)]\n",
    "    if existing_words:\n",
    "        embedding = np.zeros((len(existing_words), model.vector_size), dtype=np.float32)\n",
    "        for i, w in enumerate(existing_words):\n",
    "                embedding[i] = model.wv[w]\n",
    "        return np.mean(embedding, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "    \n",
    "# get the embeddings of the trained model for the tokens\n",
    "def get_w2v_cbow_embeddings(tokens):\n",
    "    return np.array([get_embedding(w2v_cbow, text) for text in tokens])\n",
    "\n",
    "# get the embeddings of the trained model for the tokens\n",
    "def get_w2v_skipg_embeddings(tokens):\n",
    "    return np.array([get_embedding(w2v_skipg, text) for text in tokens])\n",
    "\n",
    "def get_fasttext_embeddings(tokens):\n",
    "    return np.array([get_embedding(fasttext, text) for text in tokens])\n",
    "\n",
    "def get_glove_document_vector(text, vocab, vectors, vector_size):\n",
    "    existing_words = [word for word in text if word in vocab]\n",
    "    if existing_words:\n",
    "        embedding = np.zeros((len(existing_words), vector_size), dtype=np.float32)\n",
    "        for i, w in enumerate(existing_words):\n",
    "                embedding[i] = vectors[w]\n",
    "        return np.mean(embedding, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "    \n",
    "\n",
    "# needs tokens from same fraction of training data as the GloVe embeddings were trained with\n",
    "def get_glove_embeddings(tokens):\n",
    "    return np.array([get_glove_document_vector(text, vocab, vectors, len(vector)) for text in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fd3616",
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns vectors for specified vectorizer and data\n",
    "def get_Vectors(vectorizer_name, df_data):\n",
    "        \n",
    "        if vectorizer_name == \"tfidf_vectors\":\n",
    "            return get_tfid_vectors(df_data)\n",
    "        \n",
    "        elif vectorizer_name == \"count_vectors\":\n",
    "            return get_count_vectors(df_data)\n",
    "            \n",
    "        elif vectorizer_name == \"w2v_cbow\":\n",
    "            train_tokens = get_train_tokens(df_data)\n",
    "            return get_w2v_cbow_embeddings(train_tokens)\n",
    "            \n",
    "            \n",
    "        elif vectorizer_name == \"w2v_skipg\":\n",
    "            train_tokens = get_train_tokens(df_data)\n",
    "            return get_w2v_skipg_embeddings(train_tokens)\n",
    "            \n",
    "        elif vectorizer_name == \"fasttext\":\n",
    "            train_tokens = get_train_tokens(df_data)\n",
    "            return get_fasttext_embeddings(train_tokens, wordEmbedding_std_parameters)\n",
    "            \n",
    "        elif vectorizer_name == \"GloVe\":\n",
    "            train_tokens = get_train_tokens(df_data)\n",
    "            return get_glove_embeddings(train_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48028ba",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389aca43",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab54ba4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning\n",
    "def param_search(vector_matrix, vector_matrix_name, estimator):\n",
    "    \n",
    "    estimator_name = estimator['name']\n",
    "    parameters = estimator['parameters']\n",
    "    estimator = estimator['estimator']\n",
    "    \n",
    "    # configure the cross-validation procedure\n",
    "    cv_outer = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    \n",
    "    # enumerate splits\n",
    "    outer_results = list()\n",
    "    X = vector_matrix\n",
    "    y = train_target['label']\n",
    "    \n",
    "    for train_ix, test_ix in cv_outer.split(X,y):\n",
    "        \n",
    "        # split data\n",
    "        X_train, X_test = X[train_ix, :], X[test_ix, :]\n",
    "        y_train, y_test = y[train_ix], y[test_ix]\n",
    "    \n",
    "        # specify the nested cross validation\n",
    "        nested_cv = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)\n",
    "\n",
    "        # create the grid search instance\n",
    "        search = GridSearchCV(estimator, parameters, scoring='accuracy', cv=nested_cv, n_jobs=-1)\n",
    "        \n",
    "        # execute search\n",
    "        result = search.fit(vector_matrix, train_target['label'])\n",
    "        \n",
    "        # get the best performing model fit on the whole training set\n",
    "        best_estimator = result.best_estimator_\n",
    "        \n",
    "        # evaluate model on the hold out dataset\n",
    "        predictions = best_estimator.predict(X_test)\n",
    "        \n",
    "        # evaluate the model\n",
    "        acc = accuracy_score(y_test, predictions)\n",
    "        \n",
    "        # store the result\n",
    "        outer_results.append(acc)\n",
    "        \n",
    "        # report progress\n",
    "        print('>acc=%.3f, est=%.3f, cfg=%s' % (acc, result.best_score_, result.best_params_))\n",
    "        \n",
    "    # summarize the estimated performance of the model\n",
    "    accuracy_mean = np.mean(outer_results)\n",
    "    \n",
    "    #print the best parameter setting\n",
    "    print(\"Classifier: {}\".format(estimator_name))\n",
    "    print(\"Vectorizer: {}\".format(vector_matrix_name))\n",
    "    print(\"Mean Accuracy: {}\".format(accuracy_mean))\n",
    "    print()\n",
    "    \n",
    "    return accuracy_mean, best_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792aff84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune for the best vectorizer-estimator combination\n",
    "def hyper_tune():\n",
    "    \n",
    "    kNeighbors_parameters = {\n",
    "        'n_neighbors': [10,12,14] \n",
    "    }\n",
    "    \n",
    "    decisionTree_parameters = {\n",
    "        'criterion':['gini', 'entropy'], \n",
    "        'max_depth': [40,60,80,100],\n",
    "        'min_samples_split' : [20,30,40]\n",
    "    }\n",
    "    \n",
    "    \n",
    "    GaussianNB_parameters = {       \n",
    "\n",
    "    }\n",
    "    \n",
    "    svc_parameters = {\n",
    "        'gamma': ['auto', 'scale'],\n",
    "        'kernel': ['rbf','sigmoid']\n",
    "    }\n",
    "    \n",
    "    mlpClassifier_parameters = {\n",
    "        'activation': ['tanh', 'relu'],\n",
    "        'solver': ['adam', 'lbfgs'],\n",
    "        'hidden_layer_sizes':[(100,), (100,100), (50,), (50,50)],\n",
    "    }\n",
    "    \n",
    "    rfc_parameters = {\n",
    "        'max_depth': [40,60,80,100], \n",
    "        'max_features': ['sqrt'],\n",
    "        'n_estimators': [100,200,300]\n",
    "    } \n",
    "    \n",
    "    \n",
    "    # ------ VECTORIZERS ------\n",
    "    train_tokens = get_train_tokens(train_data)\n",
    "    \n",
    "    vectorizer_names = [\"tfidf_vectors\", \"count_vectors\", \"w2v_cbow\", \"w2v_skipg\", \"fasttext\", \"GloVe\"]\n",
    "\n",
    "    vectorizer_values = [\n",
    "        get_tfid_vectors(train_data), get_count_vectors(train_data),\n",
    "        get_w2v_cbow_embeddings(train_tokens), get_w2v_skipg_embeddings(train_tokens),\n",
    "       get_fasttext_embeddings(train_tokens),get_glove_embeddings(train_tokens)\n",
    "    ]\n",
    "\n",
    "    del train_tokens\n",
    "    vectorizers = dict(zip(vectorizer_names, vectorizer_values))\n",
    "    print('vectorizers calculated')\n",
    "    \n",
    "\n",
    "    \n",
    "    # ------ ESTIMATORS ------\n",
    "    #TODO: add more estimators\n",
    "    estimators = {\n",
    "        'KNeighborsClassifier': { 'name': 'KNeighborsClassifier', 'estimator': KNeighborsClassifier(), 'parameters': kNeighbors_parameters },\n",
    "        'DecisionTreeClassifier': { 'name': 'DecisionTreeClassifier', 'estimator': DecisionTreeClassifier(), 'parameters': decisionTree_parameters },\n",
    "        'GaussianNB': { 'name': 'GaussianNB', 'estimator': GaussianNB(), 'parameters': GaussianNB_parameters },\n",
    "        'SVC': { 'name': 'SVC', 'estimator': SVC(probability=True), 'parameters': svc_parameters },\n",
    "        'MLPClassifier': { 'name': 'MLPClassifier', 'estimator': MLPClassifier(max_iter=500), 'parameters': mlpClassifier_parameters },\n",
    "        'RandomForestClassifier': { 'name': 'RandomForestClassifier', 'estimator': RandomForestClassifier(), 'parameters': rfc_parameters }\n",
    "\n",
    "    }\n",
    "    \n",
    "    \n",
    "    best_score = 0\n",
    "    for vectorizer in vectorizers:\n",
    "        for estimator in estimators:\n",
    "            if not ((vectorizer == 'tfidf_vectors' or vectorizer == 'count_vectors') and estimator == 'GaussianNB'):\n",
    "                score, model = param_search( vectorizers[vectorizer], vectorizer, estimators[estimator] )\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_estimator_name = estimator\n",
    "                    best_vectorizer_name = vectorizer\n",
    "                    best_estimator = model\n",
    "\n",
    "    print(\"\\nThe best performance is reached with the estimator \" + best_estimator_name + \" and the vectorizer \" + best_vectorizer_name + \" with an accuracy of \" + str(best_score) )\n",
    "    return best_estimator, best_vectorizer_name, vectorizers\n",
    "\n",
    "\n",
    "#the best performing model and vectorizer\n",
    "estimator, vectorizer_name, vectorizers = hyper_tune()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f360adaf",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning for Vectorizors on a given classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a316395",
   "metadata": {},
   "source": [
    "##### For the best model, vectorizer combination execute vectorizer hyperparameter tuning (not for GloVe, since its trained in C)\n",
    "##### Only provides local optimum for the provided classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01905004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer_name is one of the following 'w2v_cbow' / 'w2v_skipg' / 'fasttext' / 'tfidf_vectors' / 'count_vectors'\n",
    "# returns best parameter combination for vectorizer 'vectorizer_name' with given classifier\n",
    "def hypertuneVectorizor(estimator, vectorizer_name):\n",
    "    \n",
    "    best_acc = 0\n",
    "    best_model = {}\n",
    "    parameters = ParameterGrid({})\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(train_data, train_target, test_size=0.2, random_state=22)\n",
    "    \n",
    "    if vectorizer_name == 'tfidf_vectors' or vectorizer_name == 'count_vectors':\n",
    "        \n",
    "        parameters = ParameterGrid({\n",
    "        'ngram_range': [(1,1), (1,2)],\n",
    "        'max_features': [10000, 30000, 50000, 80000],\n",
    "        'min_df': [1, 2, 5]\n",
    "        })\n",
    "        \n",
    "    \n",
    "    elif vectorizer_name == 'GloVe':\n",
    "        return None\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        parameters = ParameterGrid({\n",
    "        'min_count': [5, 8],\n",
    "        'vector_size': [50, 100, 200],\n",
    "        'window': [5, 8, 10]\n",
    "        })\n",
    "        \n",
    "        train_df_tokens = get_train_tokens(train_df)\n",
    "        train_tokens = get_train_tokens(X_train)\n",
    "        test_tokens = get_train_tokens(X_test)\n",
    "\n",
    "\n",
    "    print(\"Hyperparametertuning for Vectorizer\", vectorizer_name, \"given estimator\", estimator, \"\\n\")\n",
    "    for param_comb in parameters:\n",
    "         \n",
    "        if vectorizer_name == 'tfidf_vectors':\n",
    "            \n",
    "            vec_model = TfidfVectorizer(**param_comb)\n",
    "            vec_model.fit_transform(train_df['text'].values.ravel())\n",
    "            \n",
    "            emb_train = vec_model.transform(X_train['text'])\n",
    "            emb_test = vec_model.transform(X_test['text'])\n",
    "                \n",
    "        elif vectorizer_name == 'count_vectors':\n",
    "                \n",
    "            vec_model = CountVectorizer(**param_comb)\n",
    "            vec_model.fit_transform(train_df['text'].values.ravel())\n",
    "            \n",
    "            emb_train = vec_model.transform(X_train['text'])\n",
    "            emb_test = vec_model.transform(X_test['text'])\n",
    "            \n",
    "        elif vectorizer_name == 'w2v_cbow':\n",
    "                \n",
    "            vec_model = Word2Vec(train_df_tokens, **param_comb)\n",
    "            emb_train = np.array([get_embedding(vec_model, text) for text in train_tokens])\n",
    "            emb_test = np.array([get_embedding(vec_model, text) for text in test_tokens])\n",
    "                \n",
    "        elif vectorizer_name == 'w2v_skipg':\n",
    "            \n",
    "            vec_model = Word2Vec(train_df_tokens, **param_comb, sg=1)\n",
    "            emb_train = np.array([get_embedding(vec_model, text) for text in train_tokens])\n",
    "            emb_test = np.array([get_embedding(vec_model, text) for text in test_tokens])\n",
    "            \n",
    "                \n",
    "        elif vectorizer_name == 'fasttext':\n",
    "\n",
    "            vec_model = FastText(**param_comb)\n",
    "            vec_model.build_vocab(corpus_iterable=train_df_tokens)\n",
    "            vec_model.train(corpus_iterable=train_df_tokens, total_examples=len(train_df_tokens), epochs=10)\n",
    "            \n",
    "            emb_train = np.array([get_embedding(vec_model, text) for text in train_tokens])\n",
    "            emb_test = np.array([get_embedding(vec_model, text) for text in test_tokens])\n",
    "                \n",
    "                \n",
    "        model = estimator.fit(emb_train, y_train['label'])\n",
    "        predictions = model.predict(emb_test)\n",
    "        probabilities = model.predict_proba(emb_test)\n",
    "\n",
    "        \n",
    "        acc = accuracy_score(y_test['label'].to_numpy(), predictions)\n",
    "        print(\"Accuracy:\", round(acc,2), \"   Parameter Combination:\", param_comb)\n",
    "        \n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_model = {'vectorizer': vectorizer_name, 'parameters': param_comb, 'acc': best_acc, 'model': vec_model}\n",
    "            best_param = param_comb\n",
    "    \n",
    "    print(\"Best parameter combination:\", best_param)\n",
    "    return best_model, predictions, probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ce0a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model, predictions, probabilities = hypertuneVectorizor(estimator, vectorizer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dc3780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reassign the vectorizer model with new local optimum\n",
    "if vectorizer_name == 'tfidf_vectors':\n",
    "    vectorizer_tf_idf = best_model['model']\n",
    "    \n",
    "elif vectorizer_name == 'count_vectors':\n",
    "    vectorizer_count = best_model['model']\n",
    "    \n",
    "elif vectorizer_name == 'w2v_cbow':\n",
    "    w2v_cbow = best_model['model']\n",
    "\n",
    "        \n",
    "elif vectorizer_name == 'w2v_skipg':\n",
    "    w2v_skipg = best_model['model']\n",
    "\n",
    "        \n",
    "elif vectorizer_name == 'fasttext':\n",
    "    fasttext = best_model['model']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c253b4",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee25fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in preprocessed test data if necessary\n",
    "test_df = pd.read_csv('preprocessed_test_data.csv', sep=';', encoding='utf-8')\n",
    "\n",
    "test_df.loc[(test_df.label == 0),'label']='World'\n",
    "test_df.loc[(test_df.label == 1),'label']='Sports'\n",
    "test_df.loc[(test_df.label == 2),'label']='Business'\n",
    "test_df.loc[(test_df.label == 3),'label']='Sci/Tech'\n",
    "\n",
    "test_df['label']= test_df['label'].astype(str)\n",
    "\n",
    "test_data, test_target = balanced_fractionize(test_df, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e58ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model on the specified fraction of training data\n",
    "def train_model(df_data, df_target, vectorizer_name):\n",
    "    return estimator.fit(get_Vectors(vectorizer_name, df_data), df_target['label'])\n",
    "\n",
    "\n",
    "#make predictions\n",
    "def predict(train_data, train_target, vectorizer_name, estimator):\n",
    "\n",
    "    #get the trained model\n",
    "    model = train_model(train_data, train_target, vectorizer_name)    \n",
    "\n",
    "    return model.predict(get_Vectors(vectorizer_name, test_data)), model.predict_proba(get_Vectors(vectorizer_name, test_data))\n",
    "\n",
    "\n",
    "predictions, probs = predict(train_data, train_target, vectorizer_name, estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0655ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shows results from evaluation metrics\n",
    "def evaluation_figures(predictions, probs, test_target):\n",
    "    \n",
    "    # get all unique classes\n",
    "    classes = np.unique(test_target)\n",
    "\n",
    "    # evaluate the model\n",
    "    accuracy = accuracy_score(test_target['label'].to_numpy(), predictions)\n",
    "    auc_score = roc_auc_score(test_target['label'].to_numpy(), probs, multi_class=\"ovr\")\n",
    "    print(\"Model: \", )\n",
    "    print(\"Accuracy:\",  round(accuracy,2))\n",
    "    print(\"Auc:\", round(auc_score,2))\n",
    "    print(\"Detail:\")\n",
    "    print(classification_report(test_target, predictions))\n",
    "\n",
    "    ## Plot confusion matrix\n",
    "    cm = confusion_matrix(test_target, predictions)\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, \n",
    "                cbar=False)\n",
    "    ax.set(xlabel=\"Pred\", ylabel=\"True\", xticklabels=classes, \n",
    "           yticklabels=classes, title=\"Confusion matrix\")\n",
    "    plt.yticks(rotation=0)\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "\n",
    "    ## Plot roc\n",
    "    for i in range(len(classes)):\n",
    "        fpr, tpr, thresholds = roc_curve(test_target['label'].to_numpy(),  \n",
    "                               probs[:,i], pos_label=classes[i])\n",
    "        ax[0].plot(fpr, tpr, lw=3, \n",
    "                  label='{0} (area={1:0.2f})'.format(classes[i], \n",
    "                                  metrics.auc(fpr, tpr))\n",
    "                   )\n",
    "    ax[0].plot([0,1], [0,1], color='navy', lw=3, linestyle='--')\n",
    "    ax[0].set(xlim=[-0.05,1.0], ylim=[0.0,1.05], \n",
    "              xlabel='False Positive Rate', \n",
    "              ylabel=\"True Positive Rate (Recall)\", \n",
    "              title=\"Receiver operating characteristic\")\n",
    "    ax[0].legend(loc=\"lower right\")\n",
    "    ax[0].grid(True)\n",
    "\n",
    "    ## Plot precision-recall curve\n",
    "    for i in range(len(classes)):\n",
    "        precision, recall, thresholds = precision_recall_curve(\n",
    "                     test_target['label'].to_numpy(), probs[:,i], pos_label=classes[i])\n",
    "        ax[1].plot(recall, precision, lw=3, \n",
    "                   label='{0} (area={1:0.2f})'.format(classes[i], \n",
    "                                      metrics.auc(recall, precision))\n",
    "                  )\n",
    "    ax[1].set(xlim=[0.0,1.05], ylim=[0.0,1.05], xlabel='Recall', \n",
    "              ylabel=\"Precision\", title=\"Precision-Recall curve\")\n",
    "    ax[1].legend(loc=\"best\")\n",
    "    ax[1].grid(True)\n",
    "\n",
    "\n",
    "    fig.set_figheight(5)\n",
    "    fig.set_figwidth(15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50541605",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "evaluation_figures(predictions, probs, test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ef499c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
